{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VA_model 2\n",
    "### copy from ~/Desktop/P/pkl_to_csv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as P\n",
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23510"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"load dataset.pkl\"\n",
    "base_dir = P(\"total_dataset\")\n",
    "assert base_dir.is_dir()\n",
    "\n",
    "file_dir = base_dir/\"dataset.pkl\"\n",
    "\n",
    "with open(file_dir,\"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('伴侶：臭貓貓 我玩遊戲的時候一直煩我，罵他還一直蹭我😡\\n（還是伴侶）\\n伴侶：我早上幫他乾洗澡後他就不讓我摸摸了😭', 'anger', '😡')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\n",
    "    \"sadness\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"love\": 2,\n",
    "    \"anger\": 3,\n",
    "    \"fear\": 4,\n",
    "    \"surprise\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"4 columns, (text, emotion, label (int), emoji\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use pickle directly, since csv keep running into key error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('伴侶：臭貓貓 我玩遊戲的時候一直煩我，罵他還一直蹭我😡\\n（還是伴侶）\\n伴侶：我早上幫他乾洗澡後他就不讓我摸摸了😭', 'anger', '😡'),\n",
       " 23510)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Try from reading pure python list and manipulate with dataloader\"\n",
    "import pickle\n",
    "\n",
    "with open(base_dir/\"dataset.pkl\", \"rb\") as f:\n",
    "    data_list = pickle.load(f)\n",
    "\n",
    "data_list[0], len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('伴侶：臭貓貓 我玩遊戲的時候一直煩我，罵他還一直蹭我😡\\n（還是伴侶）\\n伴侶：我早上幫他乾洗澡後他就不讓我摸摸了😭', 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = [(ele[0], label2int[ele[1]]) for ele in data_list]\n",
    "\n",
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx][0], self.data[idx][1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = D_Dataset(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('伴侶：臭貓貓 我玩遊戲的時候一直煩我，罵他還一直蹭我😡\\n（還是伴侶）\\n伴侶：我早上幫他乾洗澡後他就不讓我摸摸了😭', 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(d,shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('伴侶：臭貓貓 我玩遊戲的時候一直煩我，罵他還一直蹭我😡\\n（還是伴侶）\\n伴侶：我早上幫他乾洗澡後他就不讓我摸摸了😭',\n",
       "  '😡\\n\\n好一天又罷工，我的腰好不爭氣',\n",
       "  '有一種，終於結束的感覺了嗎？🤔\\n\\n在我之前的噗文裡，有一個叫「家蓉緋緋」的，\\n她當時在我還就職在寵物學校裡時，就讓我發現他是個很愛裝「我是無辜無害的」女人。\\n\\n她曾跟我說：「以前當學生時，都被其他人霸凌」\\n而我後來也跟他說：「你被霸凌是應該的，因為你處處要跟每個人好，你每個人都拼命想討好，所以你會被霸凌是正常的！」\\n\\n後來我離開寵美學校後，我也不再與他們聯絡，打死我都不會再聯絡的😡\\n\\n唯一一次的聯絡，是在去年傳了這個訊息給我😡\\n\\n後來這個截圖我拿去詢問較好的同行朋友，每個人都在說：「這個人是神經病嗎？他有問題是不是？分不清責任歸屬？」\\n\\n為何我會說這事情彷彿告一段落的原因是⋯這女人因為朋友關係，又退出所有學校相關的群組了👍',\n",
       "  '[日韓擔的最怒] [知韓知日，學優明缺]\\n@enfihu - Kakao 想戰 Google ? 也是有影響到數位音樂平台付費喔！ 我有在ptt韓...\\n/ 前情提要\\n—\\n我開始覺得KBS 對不朽名曲/柳喜烈寫生簿/開放音樂會演出內容，管非韓國IP、是否更換/釋出多角度影片的『彈性標準』感到火大😡🤬\\n外飯的瀏覽權不重要嗎？ - 當然也有Content ID /版權等等因素\\n把我們當提款機嗎！',\n",
       "  '中國「飛彈首次穿越台灣」 國防部告訴你為何不擊落、不公布、沒警報｜蘋果新聞網｜蘋果日報\\n真的有飛彈從上空飛過去 從上面飛過去這就過分了吧😡',\n",
       "  '薯的第四十七/八天\\n\\n回家果然沒什麼動草 然後不知道為什麼一直把飼料推出去😡\\n馬上就拆盈盈的葉多\\n雖然顏色沒有很翠綠了 但真的葉多！幾乎整包都葉子🤣薯的接受度蠻高的好棒\\n而且可能因為都是葉子 所以他講他也有多吃一點梗\\n明天會再放一點花穗 因為感覺他對葉子要求很高⋯明明就一堆在那邊還是不想吃\\n\\n今天幫她大掃除 真的是亂尿尿到連巧拼上都有\\n但整個換完後他到目前都沒有亂尿尿了\\n所以可能真的不能偷懶要幫他每天換便盆\\n最近在考慮要不要請天友做墊子\\n這樣薯趴在那邊吃草時才不會碰到尿尿 還在觀察他這兩天抓癢的情況 就是眼周跟下半身居多 目前只先推測是冷氣吹到太乾 因為沒有看到他身上有蟲蟲或黑點點 但我也不知道怎麼解決這個問題 再看看 阿薯今天很活潑還有跑跑 剛好錄到最後一段🥰',\n",
       "  '天馬司快打下去😡\\n@LengLing - 類司',\n",
       "  '冷氣就是我baby 一度5塊是什麼東西😡\\n\\n謝謝希珥發起活動🥺💕希望大家喜歡！\\n幫佐藍刷觀看→\\n夏日與你－佐藍 with. 希珥 #Shorts【#OpenVerseChallengeTW #夏日與你...\\n《Open Verse Challenge - 夏日與你 》\\n參與辦法：Update your browser to use Google Drive, Docs, Sheet...\\n編曲：jupabeats\\n\\n#OpenVerseChallengeTW #夏日與你\\n#台V #ZolarLIVE #比鄰星域 #佐藍',\n",
       "  '覺得爸媽口氣真的很有問題，要我煮飯是沒關係\\n但一開口就是「怎麼還沒煮！」「是不會煮飯喔！」整個聽起來就是很不爽，會覺得你們不會自己煮啊😡\\n反倒是哥哥說「幫忙煮一下飯喔。」我就不會覺得怎樣 然後就會去煮了',\n",
       "  '蛤？帥到生氣😡',\n",
       "  '很努力讓自己不要心情不好，可是下雨就是很煩！！尤其是還打亂原本計畫的時候😡',\n",
       "  '男人不要在我床上對峙😡',\n",
       "  '大家七夕快樂 早安您好今天寫文沒？（長輩圖問候）\\n\\n我被朋友的梗圖弄到爆笑\\n（中國梗圖，慎）\\n\\n順帶一提，偷看別人洗澡違反社維法、偷別人衣服是竊盜罪ㄛ！😡',\n",
       "  '為什麼你們都有對象啊，你們對象不應該是我嗎😡快跟我說快樂啊😡🌹',\n",
       "  '【原神整活】班尼特：😡rnm旅行者！我不干了！你这契约有问题啊？！😡\\n笑死w 真六星工具人過勞火神',\n",
       "  '♦️8/3《 #獅子藏匿的書屋 》第十四回\\n《獅子》復刊啦！\\n小冬因為一時衝動作了不可挽回（？）的事！（雖然GO看得很開心\\n結果因為太羞愧就跑了，天啊這樣叫夏生如何是好 但不愧是追了老師多年的粉絲（？），總是可以找到她在哪\\n#如果心意相通\\n#也許就能下出一盤精彩的聯棋吧\\n#夏生是不是知道自己很帥搞什麼鬼？？？😡（氣什麼\\n👉連載任意門：GO原漫基地｜獅子藏匿的書屋\\n🗣漫畫： #小島',\n",
       "  '#純屬好奇\\n\\n讓整個台灣離開臉書轉移使用的社群平台，需要什麼條件🤔\\n\\nmurmur臉書真的好煩😡',\n",
       "  '我愛死辣椒的反應跟談吐😡❤️🌶️\\n#515 #vtuber',\n",
       "  '薯的第四十六天\\n\\n在只剩下很多梗的袋子裡挖草\\n我盡力了⋯學到教訓以後不要吝嗇於買草這件事\\n今天又下單了噗啾的500克葉多\\n希望可以撐到8/18免運哈哈哈\\n\\n薯今天感覺心情有好一點\\n但還是亂尿尿欸😡\\n為什麼呢阿薯薯\\n\\n一直跟他說好吃的草草明天就來了忍耐一下\\n希望他聽得懂\\n\\n晚上餵菜菜 青椒咬了兩口就不要了⋯一直試圖爬上來找其他的 大陸妹不管餵多少都還是狂熱愛好 我也是\\U0001fae0\\U0001fae0 他這幾天水都喝很多 幾乎倒105晚上都可以剩35以下 飼料沒有吃完 可能是加了他不想要的就有味道就不吃 我又要尋找新的飼料了嗎⋯',\n",
       "  '尊重我喔😡\\n#反串要註明',\n",
       "  '永和樂華開了一間，\\n讓我想起我跟親友的對話\\n－\\n我：我有喝過某個網紅他妹的開的飲料店，叫做「再睡５秒鐘」。\\n菲比、ET：再睡5分鐘啦!!5秒鐘是睡屁喔!!!😡😡😡\\n\\n一句話引起眾怒，\\n怕。😨',\n",
       "  'ガルパコラボ!ってなっても九州は対象外だったりするからまじで💩\\n全国でやれ😡🔥 ',\n",
       "  '【聯動💙💗超級兔人 8/2 21:00】\\n超級兔人，我ㄉ69超人\\n把你ㄉ紅蘿蔔交出來😡(文案這樣寫真的OKㄇ?)\\n待機室→\\n【超級兔子人】傳說中ㄉ69😳KOZMII粉藍組💙💗SUPER BUNNY MAN ft.@莉莉 埃絲忒L...\\n#superbunnyman #比鄰星域 #超級兔子人 #新人vtuber #台v #KOZMII #台灣vtuber #ProximaSector #ZolarLIVE #佐藍',\n",
       "  '氣死 會限沒通知 youtube🤬🤬🤬🤬🤬🤬🤬😡',\n",
       "  '不開心😡\\n有人持續把我的愛坐搶走😡😡😡\\n把我的小角落還來😭😭😭',\n",
       "  '這則噗被標示為含有成人內容，您必須登入才能查看。',\n",
       "  '前情提要：\\n柴崎柴同學前幾天突然跟我說有東西要寄給我\\n我想說是什麼東東，還在那邊回想我是有借什麼東西給她嗎\\n結果今天去領貨拆開：\\n\\n\\n幹幹幹怎麼搞暗殺😡😡😡😡 ',\n",
       "  '薯的第四十五天\\n\\n從昨天到今天早上 薯還是慌慌張張的\\n早上起床發現他昨天晚上不知道到底做了什麼\\n像被颱風尾掃過一樣⋯\\n然後要摸他也一直躲開\\n要出門前真的很擔心他今天的狀態\\n尤其草剩七八成的梗 買的還沒到\\n\\n抱著忐忑的心情回家\\n果然草跟水都沒動多少\\n水還有40左右 但草就少1/4吧\\n就算葉子也都還有啊 飼料看起來有少 但好像是被他推到便盆裡⋯ 而且她好像開始吃膩金牌了😡 某個點又開始叫 感覺跟之前要菜菜的5151有點不像 在想是不是想找同伴或身體哪裡不舒服 可是餵了小黃瓜跟狼尾草都吃的很開心 所以身體應該是沒問題 餵完這兩種後狀態好像就好一點了 所以只是心情不好⋯？ 今天晚上繼續觀察 但他這兩天真的很愛碎碎念 不知道在念什麼欸🤣好想知道 我覺得他應該聽得懂我說的三成或多一點 真是棒薯薯\\U0001f979',\n",
       "  '錢錢，對不起了錢錢\\n（（噗浪為什麼不能用融化小黃臉😡\\n4500ㄉ書（到貨等付\\n盜筆聯名，大概會是3000+1600\\n看上的裙，小提琴我無法捨棄耶，不知道多少\\n看上的鞋1000\\n北中南三場\\n跟ㄒㄒ去咒迴咖啡',\n",
       "  '幫轉\\n歡迎轉噗\\n謝謝\\n徵教學麻豆，時段如下👇\\n2022.08/14（日）下午\\n13:00、14:00 、14:40、15:20、16:00、1640\\n\\n時間只是預估，\\n因為每個人毛量不同，\\n但也不要遲到，請算好生理期，\\n可用月亮杯或棉條不影響，\\n可配合教學需求拍攝記錄（不漏臉和私處）， #需12週以上沒有處理過毛髮。 孕媽咪可應徵，僅限1400時段， 單胞胎，當日週數介於12~27+6。 雙胞胎，當日週期介於12~24+6。 費用：無，需押金500離開時退還， 放鳥者押金不退😡 ‼️需打完三劑疫苗，中國疫苗不算 ‼️居家工作室，介意有孩子勿約 🏠高雄市苓雅區（近英明市場、英明國中） #熱蠟除毛 #徵免費模特兒 #需要押金離開退還',\n",
       "  '【直播通知👽洗咧雜談 8/1 15:00】\\n地球上有個東西叫做累累病😡\\n禮拜一的症狀會最嚴重，請酌量服用佐藍直播\\n待機室→\\n【洗咧雜談】週一累累病，絕讚感染中\\n#新人VTUBER #台灣VTUBER #台V #ZolarLIVE #比鄰星域 #PowerWash #高壓清洗機',\n",
       "  '申訴的deadline還剩2天\\n申訴書總算寫完一半\\n晚上都失眠 好生氣 一定要過😡',\n",
       "  '不好意思親友看到生氣忍不住出來想講兩句👿\\n會被說護航嗎 不曉得 算了🤷\\u200d♀🤷\\u200d♀ 反正我來逆風😋🖐\\n\\n嫌棄我們家醜圖妹👧畫的圖不夠價的旅人們不知道是什麼意思耶😍😍😍👌\\n你不喜歡那是你的事情🙈 不代表別人不喜歡啊🥺🥺🥺\\n交易本來就是雙方你情我願的事情🥴更何況這噗本來也就不是轉蛋或鑑價 你喜歡你就委💕 不喜歡就滑掉不好ㄇ🤭🤭\\n硬要在這邊指教 那你各位旅人真的都很會畫畫椰！🥳🥳🥳👊\\n\\n你不覺得這值一幣 但也會有人覺得值ㄚ😒 說不值就不值？人家畫畫也要時間餒 😡😡更何況噗幣也就是1起跳 沒有什麼0.5之類的（太久沒用噗浪ㄌ應該沒錯吧⋯）🥴👋\\n\\n啊直接留言吃瓜的也不知道是什麼心態==🤮🤮👎',\n",
       "  'zø\\n\\n為什麼！！！我就是抽不到裙子😡😡😡😡',\n",
       "  'Stray：想說今天有點時間要跑個進度，結果一直死在工廠好氣😡',\n",
       "  '聖子祭/再錄集2/舊圖重發系列\\n#原創BL #互攻 #聖子祭\\n\\n你這個木頭人！😡\\U0001fab5\\n上色後才驚覺古這樣穿真的是太火辣了⋯⋯跟天氣一樣毒辣⋯難怪里要擔心成這樣',\n",
       "  '「これ以上来ないで！叫びますよ！😡」\\nと、可愛く威嚇しているビビり症の大猫くん。\\n（キャラ紹介風にしてます。まだ途中です。）\\n\\n「不要再過來！不然我要叫了喔！😡」\\n膽小怕事的大貓君如此可愛地威嚇著。\\n（正在弄成角色介紹風的途中） ',\n",
       "  '一天只能收認證簡訊5次，真的麻煩，害我不能用卡😡',\n",
       "  '我不去看雷神索爾ㄌ\\n手上所有的電影優待券都要拿來買這部電影的票\\n有幾場看幾場\\n請假去從早上看到晚上😡\\n【獨家】台灣確定引進《語意錯誤》電影版！ 韓國首映導演搶票也失敗',\n",
       "  '😡可愛',\n",
       "  'Wordle 406 X/6\\n\\n\\U0001f7e9⬛⬛⬛⬛\\n\\U0001f7e9\\U0001f7e8⬛⬛⬛\\n\\U0001f7e9⬛\\U0001f7e9⬛⬛\\n\\U0001f7e9⬛⬛⬛⬛\\n\\U0001f7e9\\U0001f7e9\\U0001f7e9⬛⬛\\n\\U0001f7e9\\U0001f7e9\\U0001f7e9⬛⬛\\n\\n😡\\U0001fae0',\n",
       "  '我畫畫畫到碗跟衣服都他媽的還沒洗\\n乾😡😡😡😡😡',\n",
       "  '😡',\n",
       "  'OK事情是這樣ㄉ\\n森林ㄉ動物們去聚餐活動🍷\\n為什麼大象🐘突然生氣😡？\\n.\\n.\\n.\\n.\\n.\\n.\\n.\\n. . . . . . . . 原來 這是一個氣象局...🐘🤬',\n",
       "  '花卷是我的神😡😡',\n",
       "  '乾你地區限制😠😠😠讓大家輕鬆點聽我們孫溫蒂唱歌不好嗎😡害我隨時想聽也一定要開vpn',\n",
       "  '卯咪一直想要霸佔電腦😡\\n桌子什麼時候才會來啊😢',\n",
       "  '先這樣 晚上再擦一遍😡😡😡😡😡',\n",
       "  '我真的要生氣ㄌ\\n為什麼之前不好好練骨架跟肌肉\\n現在要練我就像個小笨蛋一樣要從頭開始😡😡😡',\n",
       "  '氣炸鍋😡💥💥🤬',\n",
       "  '🦊因為⛰️比他高突然鬧彆扭\\n⛰️：我今天沒有鞋墊啊\\n🦊：我知道你沒有😡\\n⛰️：我知道了我就這樣（蹲下來配合🦊的身高）\\n🦊：哈吉嘛你幹嘛啦！！！（又踢了⛰️）\\n哈哈哈最難討好的5歲榮 仗著⛰️寵他',\n",
       "  '薯的第四十天\\n\\n媽媽今天出門工作了 有點擔心他又不吃飯\\n結果其實還不賴 飼料幾乎吃完 草有剩一半但葉子都沒了\\n只有水沒什麼動 才35😡我想主因之一是嘴的方向不是他趴著那邊⋯\\n\\n回家後又幫他加了一半的oxbow\\n到九點多吃菜菜時只剩一兩口了\\n今天有大陸妹兩片跟青椒1/4\\n明明只有昨天沒給他 像餓幾百年一樣 一下就吃完了\\n誇張的是吃完還繼續去吃草跟飼料\\n後來趴在那邊的時候肚子圓滾滾的超好笑哈哈哈\\n感覺最近真的有變大隻一點好棒😻 晚上外面關了冷氣後還是感覺有點熱 所以就移進房間了 希望他今天不要再在南瓜尿尿了 另外一個得意的事是 阿薯現在還是只有我可以自由摸摸嘿嘿 一旦發現是其他人就會馬上跳開 媽媽最近不知道哪裡得罪他一摸就躲開好好笑',\n",
       "  '手機拍不了可愛的貓貓😡\\n現實比照片可愛一百倍',\n",
       "  '哪有一次出這麼多活動是要怎麼玩😡',\n",
       "  '太生氣了😡\\n\\nコロナバカヤロー！！！！！！！！！',\n",
       "  '正常的狗狗很可愛 😚\\n壞掉的狗狗就像圖片 😡\\n認真覺得蘋果畫得很真實 👍\\n看到重機就眼紅(忌妒)\\n看到重機就狂吠(酸)\\n看到重機就想咬(撞)\\n看到重機就牙起來(大爺個性)',\n",
       "  '窗外有野貓在叫囂 （被吵醒\\n好生氣喔😡',\n",
       "  '到底哪些猴子敢嘴我pika cream….😡\\n\\n本來想說要去看現場的\\n希望他們能夠繼續勇敢追夢',\n",
       "  '月經真的很欠揍⋯😡囁く',\n",
       "  '為什麼哥哥可以那麼可愛啊😡😡\\n ',\n",
       "  '趕快更新你的抖音啊中二大東💢我都下載整整兩天了還沒動靜 我要看直播😡',\n",
       "  '我的噗浪有時候都不跳通知，然後都到我自己點開通知欄才發現我錯過了一個世紀😡😡😡 ',\n",
       "  '打會武不要用滅火隊😡\\n打奇遇給我帶四藝雞😡\\n\\n#花亦山心之月 #反串要註明',\n",
       "  'A3! ちびぐるみ第二弾～秋組～｜商品情報｜バンプレストナビサイト シェアしよう\\n\\n我要生氣ㄌ 又出這個😡😡😡😡'),\n",
       " tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### play around the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.05, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (2): Mish()\n",
       "  (3): Dropout(p=0.05, inplace=False)\n",
       "  (4): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from EMO_AI.all import *\n",
    "\n",
    "base_model = get_model(pretrained=False)\n",
    "base_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]),\n",
       " tensor([[ 0.0193, -0.0264, -0.0235, -0.0358,  0.0220, -0.0137]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ = base_model(convert_text_to_tensor(\"This love\"))\n",
    "output_.shape, output_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the data to the right shape (read from DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizersCollateFn:\n",
    "    def __init__(self, max_tokens=512):\n",
    "        \"\"\"\n",
    "        from tokenizers import ByteLevelBPETokenizer\n",
    "        from tokenizers.processors import BertProcessing\n",
    "        ## RoBERTa uses BPE tokenizer similar to GPT\n",
    "        t = ByteLevelBPETokenizer(\n",
    "            \"tokenizer/vocab.json\",\n",
    "            \"tokenizer/merges.txt\"\n",
    "        )\n",
    "        t._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        t.enable_truncation(max_tokens)\n",
    "        t.enable_padding(length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n",
    "        self.tokenizer = t\"\"\"\n",
    "        self.tokenizer = get_tokenizer()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
    "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
    "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
    "        labels = torch.tensor([x[1] for x in batch])\n",
    "        \n",
    "        return (sequences_padded, attention_masks_padded), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[    0, 48341, 20024,  ...,     1,     1,     1],\n",
       "          [    0, 18636,  5543,  ...,     1,     1,     1],\n",
       "          [    0, 44636, 23171,  ..., 48537, 10278,     2],\n",
       "          ...,\n",
       "          [    0, 47876,  3602,  ...,     1,     1,     1],\n",
       "          [    0, 48548,  9085,  ...,     1,     1,     1],\n",
       "          [    0,   250,   246,  ...,     1,     1,     1]]),\n",
       "  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])),\n",
       " tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL = DataLoader(d,shuffle=False, batch_size=64, collate_fn=TokenizersCollateFn())\n",
    "\n",
    "\"(x,atten_x, y)\"\n",
    "next(iter(DL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 512]), torch.Size([64, 512]), torch.Size([64]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,at), y = tmp\n",
    "x.shape, at.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two different way to approach this problem\n",
    "Proposal 1: DummyModel()\n",
    "x: text_tensor, y: emotion (int, the 6-label thing)\n",
    "train the model, as usual, on text and label\n",
    "learn the VA layer, F layer by training\n",
    "check the output (y), to see if the VA layer, F layer helps to predict the emotion better\n",
    "\n",
    "Attention: \n",
    "the x it trains on should be A-B-A conversation \n",
    "to learn the emotion change\n",
    "\n",
    "Proposal 2:\n",
    "manually label the VA, F of each set of data\n",
    ", and train the model with label of emotion(int), VA(tensor), F(tensor)\n",
    "\n",
    "option 1: assume VA, F already affect the conversation undering hood\n",
    "...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Proposal 1\"\"\"\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "\n",
    "        \"feed s into f\"\n",
    "        \"output: f\"\n",
    "        self.s_f_layer = nn.Linear()\n",
    "\n",
    "        \"output: s\"\n",
    "        self.s_layer = nn.Linear()\n",
    "\n",
    "        \"feed x, s, va_old to get va_new\"\n",
    "        \"output: va_new\"\n",
    "        self.va_layer = nn.Linear()\n",
    "\n",
    "\n",
    "    def forward(self, x, va_old):\n",
    "        \"input VA_old and text(string)\"\n",
    "        \"tensor: x:; tensor: va_old\"\n",
    "        \"tensor: emotion\"\n",
    "        emotion = self.base_model(x)\n",
    "        \"tensor: s: affected by x and va_old\"\n",
    "        # check the dim again!\n",
    "        s_input = torch.concat((emotion, va_old), dim=1)\n",
    "        s = self.s_layer(s_input)\n",
    "\n",
    "        f = self.s_f_layer(s)\n",
    "\n",
    "        va_input = torch.concat((va_old, s, f), dim=1)\n",
    "        va_new = self.va_layer(va_input)\n",
    "        \n",
    "        \"return VA_new\"\n",
    "        return va_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('AI_sheng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9db77c7d38baf4b93e998a2213b9026a3d1545f748ab5419893b25caa4e3cb58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
