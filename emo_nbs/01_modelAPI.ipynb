{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are in dev branch so it's fine\n",
    "### On dev branch:\n",
    "###     dig into tokenized input shape\n",
    "###     check if we need to reshape/unsqueeze input for inference only\n",
    "###     don't merge this until it works\n",
    "###     probably dev on local end tho it'll takes disc space\n",
    "# run line by line instead of \"execute all\" button\n",
    "### Don't just directly re-run this notebook\n",
    "### But to copy it first\n",
    "### Running on google colab is highly recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load weight, see the saved weight inside my cloud\n",
    "## note that !wget URLs for \"model.pt\" doesn't work\n",
    "## presumably needs to unzip the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My personal note\n",
    "### torch_lr_finder: import error, which is dumb\n",
    "\n",
    "\n",
    "### Suggestion:\n",
    "#### We should try fast ai if time permits, but it's not our main goal\n",
    "\n",
    "## ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Classification using Fine-tuned BERT model\n",
    "\n",
    "In this tutorial, I will show to fine-tune a language model (LM) for emotion classification with code adapted from this [tutorial](https://zablo.net/blog/post/custom-classifier-on-bert-model-guide-polemo2-sentiment-analysis/) by MARCIN ZAB≈ÅOCKI. I adapted his tutorial and modified the code to suit the emotion classification task using a different BERT model. Please refer to his tutorial for more detailed explanations for each code block. I really liked his tutorial because of the attention to detail and the use of high-level libraries to take care of certain parts of the model such as training and finding a good learning rate. \n",
    "\n",
    "Before you get started, make sure to enable `GPU` in the runtime and be sure to \n",
    "restart the runtime in this environment after installing the `pytorch-lr-finder` library.\n",
    "\n",
    "This tutorial is in a rough draft so if you find any issues with this tutorial or have any further questions reach out to me via [Twitter](https://twitter.com/omarsar0). \n",
    "\n",
    "Note that the notebook was created a little while back so if something break it's because the code is not compatible with the library changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip these dependency shxt since we should've done it when setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers tokenizers pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you need to Restart runtime after running this code segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/davidtvs/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu113'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import logging\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import classification_report\n",
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pretrained Language Model\n",
    "We are first going to look at pretrained language model provided by HuggingFace models. We will use a variant of BERT, called DistilRoBERTa base. The `base` model has less parameters than the `larger` model. \n",
    "\n",
    "[RoBERTa](https://arxiv.org/abs/1907.11692) is a variant of of BERT which \"*modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates*\".\n",
    "\n",
    "Knowledge distillation help to train smaller LMs with similar performance and potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the tokenizer for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419c3cb65daa4068894430fe02a0fdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89698711961f415d8891362453273919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a889fbe6d58487cb250e18c1e4e9907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eabc4f212d494aaf611d6b17d8b1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the actual model with the LM head that takes care of the prediciton for the LM. When fine-tuning we don't use the head and instead use the base model. The code below shows how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eb6e5637ee4611b6f4b9a2a14e620e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(\"distilroberta-base\")\n",
    "base_model = model.base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try out the tokenizer first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Elvis is the king of rock!\"\n",
    "enc = tokenizer.encode_plus(text)\n",
    "enc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9682, 9578, 16, 5, 8453, 9, 3152, 328, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_ids` are the numerical encoding of the tokens in the vocabulary. `attention_mask` is an addition option used when batching sequences together and you want to tell the model which tokens should be attented to ([read more](https://huggingface.co/transformers/glossary.html#attention-mask)). The attention mask information helps when dealing with variance in the size of sequences and we need a way to tell the model that we don't want to attend to the padded indices of the sequence.\n",
    "\n",
    "We are only using `input_ids` and `attention_mask`\n",
    "\n",
    "We need to also unsqueeze to simulate batch processing\n",
    "\n",
    "Using DistilBertForSequenceClassification: https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## size of representation of one of the tokens \n",
    "out[0][:,0,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Size([1, 768])` represents batch_size, number of tokens in input text (lenght of tokenized text), model's output hidden size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9682, 9578, 16, 5, 8453, 9, 3152, 2]\n",
      "<s>Elvis is the king of rock</s>\n",
      "Length: 9\n",
      "torch.Size([9, 768])\n"
     ]
    }
   ],
   "source": [
    "t = \"Elvis is the king of rock\"\n",
    "enc = tokenizer.encode_plus(t)\n",
    "token_representations = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\n",
    "print(enc[\"input_ids\"])\n",
    "print(tokenizer.decode(enc[\"input_ids\"]))\n",
    "print(f\"Length: {len(enc['input_ids'])}\")\n",
    "print(token_representations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Custom Classification head on top of LM base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Mish activiation function as in the one proposed in the original tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    import torch.nn.functional as F\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will use to do the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
    "\n",
    "        # maybe do some pooling / RNNs... go crazy here!\n",
    "\n",
    "        # use the <s> representation\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretest the model with dummy text\n",
    "We want to ensure that the model is returing the right information back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "classifier = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(enc[\"input_ids\"]).unsqueeze(0).to('cpu')\n",
    "attn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1105,  0.2375, -0.1072]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier((X, attn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.json',\n",
       " 'tokenizer/merges.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load pretrained tokenizer information\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges.txt\t\t tokenizer_config.json\tvocab.json\n",
      "special_tokens_map.json  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement CollateFN using fast tokenizers.\n",
    "This function basically takes care of proper tokenization and batches of sequences. This way you don't need to create your batches manually. Find out more about Tokenizers [here](https://github.com/huggingface/tokenizers/tree/master/bindings/python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for tokenize the user input\n",
    "class TokenizersCollateFn:\n",
    "    def __init__(self, max_tokens=512):\n",
    "\n",
    "        # I still need this to parse the input\n",
    "        # try to figure out where to store these tokens\n",
    "        # instead of re-download it every time\n",
    "        # reload is probably fine if the model is on AWS?\n",
    "        ## RoBERTa uses BPE tokenizer similar to GPT\n",
    "        t = ByteLevelBPETokenizer(\n",
    "            \"tokenizer/vocab.json\",\n",
    "            \"tokenizer/merges.txt\"\n",
    "        )\n",
    "        t._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", t.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", t.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        t.enable_truncation(max_tokens)\n",
    "        t.enable_padding(length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n",
    "        self.tokenizer = t\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n",
    "        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n",
    "        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n",
    "        labels = torch.tensor([x[1] for x in batch])\n",
    "        \n",
    "        return (sequences_padded, attention_masks_padded), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data and Preview it\n",
    "Below we are going to load the data and show you how to create the splits. However, we don't need to split the data manually becuase I have already created the splits and stored those files seperately which you can quickly download below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-04 12:51:43--  https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/ikkqxfdbdec3fuj/test.txt [following]\n",
      "--2022-07-04 12:51:43--  https://www.dropbox.com/s/raw/ikkqxfdbdec3fuj/test.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com/cd/0/inline/BoZN5Yhoh4odNGswTqol1TyJvJds0cGZJ9yCO7e_SSKoDQQHx0nGoN_RdA9eK67l3PeuntcfZsX8KXqZoBEppu28WtaRFrOtxH-93B1jkcr_2EiLwsftygxqHRXg2pRXZ_FrchY4gEo_eeLulOzD43_xh9k7kdBLZJr4_sbnNaMBxg/file# [following]\n",
      "--2022-07-04 12:51:43--  https://ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com/cd/0/inline/BoZN5Yhoh4odNGswTqol1TyJvJds0cGZJ9yCO7e_SSKoDQQHx0nGoN_RdA9eK67l3PeuntcfZsX8KXqZoBEppu28WtaRFrOtxH-93B1jkcr_2EiLwsftygxqHRXg2pRXZ_FrchY4gEo_eeLulOzD43_xh9k7kdBLZJr4_sbnNaMBxg/file\n",
      "Resolving ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com (ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
      "Connecting to ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com (ucbaec52c9833bf780517d882e40.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 206760 (202K) [text/plain]\n",
      "Saving to: ‚Äòtest.txt‚Äô\n",
      "\n",
      "test.txt            100%[===================>] 201.91K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-04 12:51:44 (3.55 MB/s) - ‚Äòtest.txt‚Äô saved [206760/206760]\n",
      "\n",
      "--2022-07-04 12:51:44--  https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/1pzkadrvffbqw6o/train.txt [following]\n",
      "--2022-07-04 12:51:44--  https://www.dropbox.com/s/raw/1pzkadrvffbqw6o/train.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com/cd/0/inline/BoZfCU-qFL4O272wpMceYJp6j6MmkFri44MaNgHq_8__o5fAk3qi5T3ldqvOwkoX1kF0L6xvZQ1nUOeNMWuG98nHrrfTF7bxZI_4VP2KdYHzRN5TErmbvcPDoGBViht2NB6bG3dBpd7WGV3HcJ2YWjPInkGY9dFf-TrHPtG13tJ4Pw/file# [following]\n",
      "--2022-07-04 12:51:45--  https://uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com/cd/0/inline/BoZfCU-qFL4O272wpMceYJp6j6MmkFri44MaNgHq_8__o5fAk3qi5T3ldqvOwkoX1kF0L6xvZQ1nUOeNMWuG98nHrrfTF7bxZI_4VP2KdYHzRN5TErmbvcPDoGBViht2NB6bG3dBpd7WGV3HcJ2YWjPInkGY9dFf-TrHPtG13tJ4Pw/file\n",
      "Resolving uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com (uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
      "Connecting to uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com (uc51396343f06df4657f9c8d6aae.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1658616 (1.6M) [text/plain]\n",
      "Saving to: ‚Äòtrain.txt‚Äô\n",
      "\n",
      "train.txt           100%[===================>]   1.58M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-07-04 12:51:45 (14.7 MB/s) - ‚Äòtrain.txt‚Äô saved [1658616/1658616]\n",
      "\n",
      "--2022-07-04 12:51:45--  https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/2mzialpsgf9k5l3/val.txt [following]\n",
      "--2022-07-04 12:51:46--  https://www.dropbox.com/s/raw/2mzialpsgf9k5l3/val.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com/cd/0/inline/BoazdkJImC4ykbNVmH67WS71XHt2qV-D4BI3xTwkahetGZRpEB3RhjhO389hRSEHfKgFi07DwAjlBq1dSvoFNJV7IwLviHTI_vLKFDpocIwKtZninyW7VlBEu8xw2B0G33tDWZVEo4GarVvUqrY5TzvrJoJ0md-TW6_t7lIfr6ibyA/file# [following]\n",
      "--2022-07-04 12:51:46--  https://ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com/cd/0/inline/BoazdkJImC4ykbNVmH67WS71XHt2qV-D4BI3xTwkahetGZRpEB3RhjhO389hRSEHfKgFi07DwAjlBq1dSvoFNJV7IwLviHTI_vLKFDpocIwKtZninyW7VlBEu8xw2B0G33tDWZVEo4GarVvUqrY5TzvrJoJ0md-TW6_t7lIfr6ibyA/file\n",
      "Resolving ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com (ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
      "Connecting to ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com (ucff127704c4ae68dba2c9549db3.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204240 (199K) [text/plain]\n",
      "Saving to: ‚Äòval.txt‚Äô\n",
      "\n",
      "val.txt             100%[===================>] 199.45K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-07-04 12:51:47 (3.53 MB/s) - ‚Äòval.txt‚Äô saved [204240/204240]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
    "!wget https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
    "!wget https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the datasets as txt files\n",
    "## EXERCISE: Change this to an address\n",
    "\n",
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n",
    "val_path = \"val.txt\"\n",
    "\n",
    "## emotion labels\n",
    "label2int = {\n",
    "  \"sadness\": 0,\n",
    "  \"joy\": 1,\n",
    "  \"love\": 2,\n",
    "  \"anger\": 3,\n",
    "  \"fear\": 4,\n",
    "  \"surprise\": 5\n",
    "}\n",
    "\n",
    "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Look at the dataset\n",
    "Below is a few code sniphets to get a good idea of the dataset we are using here. You can skip this whole subsection if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-04 12:51:47--  https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.4.18, 2620:100:6019:18::a27d:412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.4.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/607ptdakxuh5i4s/merged_training.pkl [following]\n",
      "--2022-07-04 12:51:47--  https://www.dropbox.com/s/raw/607ptdakxuh5i4s/merged_training.pkl\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com/cd/0/inline/BoagSRMtLxoafwVPfktiUwWXyPv1txKcLgIQtcfekgRGSvIvDBDFdctfIdLWCtunrOCho3rkNJlJhmpwMM8GgveZkygqHr_2Ytl0EbFpB1APVPuDpNdBYvS2ymoqBKLvuia8VDeK1F5vgaEs2n1_o-qjKgeds8LCSPTwbkN5gwFgNA/file# [following]\n",
      "--2022-07-04 12:51:47--  https://uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com/cd/0/inline/BoagSRMtLxoafwVPfktiUwWXyPv1txKcLgIQtcfekgRGSvIvDBDFdctfIdLWCtunrOCho3rkNJlJhmpwMM8GgveZkygqHr_2Ytl0EbFpB1APVPuDpNdBYvS2ymoqBKLvuia8VDeK1F5vgaEs2n1_o-qjKgeds8LCSPTwbkN5gwFgNA/file\n",
      "Resolving uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com (uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6019:15::a27d:40f\n",
      "Connecting to uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com (uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BoYQ-QiaVYsJX3HieVJ4nZJG3Ji4RacJHDO1sqP6jn0ArlQN0-SM5PO8L88eW9doWEr1KyG2I68RI1sXd6N7MfF-8atNLzoOn_7iMIrNmW47swZGtIX51IMTgljubPjss7qOhznCojRHMxW5DTlG_UXiOgvtMLVosMda1pY74-ATcgiS2cbsObhSGX7w3o9qi7Vbt0O5iReMFQhMPK44SAMgWgRDGoF_aQw3ewz25qMXq6-rfEb17a6g98vADJCtQf_d1aU-xQ093-AEfj4CPbal2HDFFBS00m1rVxaEk_hb971b_ITr-MpRHf_cdYbcWHfET9Ep3jB4taI7EIf9XEUyU8H5JofBzcxeNefmxSpVENFjyDzvbtwpr4cOMCpGVFeR6VMlkHXuZtnRcg5ZVlXQSj1OVmzg373vPmcvsKJSfw/file [following]\n",
      "--2022-07-04 12:51:48--  https://uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com/cd/0/inline2/BoYQ-QiaVYsJX3HieVJ4nZJG3Ji4RacJHDO1sqP6jn0ArlQN0-SM5PO8L88eW9doWEr1KyG2I68RI1sXd6N7MfF-8atNLzoOn_7iMIrNmW47swZGtIX51IMTgljubPjss7qOhznCojRHMxW5DTlG_UXiOgvtMLVosMda1pY74-ATcgiS2cbsObhSGX7w3o9qi7Vbt0O5iReMFQhMPK44SAMgWgRDGoF_aQw3ewz25qMXq6-rfEb17a6g98vADJCtQf_d1aU-xQ093-AEfj4CPbal2HDFFBS00m1rVxaEk_hb971b_ITr-MpRHf_cdYbcWHfET9Ep3jB4taI7EIf9XEUyU8H5JofBzcxeNefmxSpVENFjyDzvbtwpr4cOMCpGVFeR6VMlkHXuZtnRcg5ZVlXQSj1OVmzg373vPmcvsKJSfw/file\n",
      "Reusing existing connection to uc615c650bddae1f31315dbe01be.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49991846 (48M) [application/octet-stream]\n",
      "Saving to: ‚Äòmerged_training.pkl‚Äô\n",
      "\n",
      "merged_training.pkl 100%[===================>]  47.68M  60.6MB/s    in 0.8s    \n",
      "\n",
      "2022-07-04 12:51:49 (60.6 MB/s) - ‚Äòmerged_training.pkl‚Äô saved [49991846/49991846]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## helper function\n",
    "def load_from_pickle(directory):\n",
    "    return pickle.load(open(directory,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f88f0bb9590>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_from_pickle(directory=\"merged_training.pkl\")\n",
    "\n",
    "## using a sample\n",
    "data= data[data[\"emotions\"].isin(emotions)]\n",
    "\n",
    "\n",
    "data = data.sample(n=20000);\n",
    "\n",
    "data.emotions.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        20000\n",
       "emotions    20000\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been preprocessed already, using technique from this paper: https://www.aclweb.org/anthology/D18-1404/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0ef19032-e640-4f44-9aa8-a5a5ed005f07\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54470</th>\n",
       "      <td>i feel most relieved</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51736</th>\n",
       "      <td>ill talk to most anyone as long as im not feel...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49002</th>\n",
       "      <td>i feel rather listless and floppy and really o...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18208</th>\n",
       "      <td>i climbed up on that tank feeling like i belon...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85461</th>\n",
       "      <td>i read from a site stating that if we feel lon...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ef19032-e640-4f44-9aa8-a5a5ed005f07')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0ef19032-e640-4f44-9aa8-a5a5ed005f07 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0ef19032-e640-4f44-9aa8-a5a5ed005f07');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                    text emotions\n",
       "54470                               i feel most relieved      joy\n",
       "51736  ill talk to most anyone as long as im not feel...    anger\n",
       "49002  i feel rather listless and floppy and really o...  sadness\n",
       "18208  i climbed up on that tank feeling like i belon...      joy\n",
       "85461  i read from a site stating that if we feel lon...  sadness"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset index\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'anger', 'sadness', 'fear', 'surprise', 'love'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check unique emotions in the dataset\n",
    "data.emotions.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data and store into individual text files\n",
    "\n",
    "If you are using your own dataset and want to split it for training, you can uncomment the code below. Otherwise, just skip it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\n\\n# Creating training and validation sets using an 80-20 split\\ninput_train, input_val, target_train, target_val = train_test_split(data.text.to_numpy(), \\n                                                                    data.emotions.to_numpy(), \\n                                                                    test_size=0.2)\\n\\n# Split the validataion further to obtain a holdout dataset (for testing) -- split 50:50\\ninput_val, input_test, target_val, target_test = train_test_split(input_val, target_val, test_size=0.5)\\n\\n\\n## create a dataframe for each dataset\\ntrain_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\\nval_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\\ntest_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\\nfinal_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\\n\\ntrain_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\\nval_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\\ntest_dataset.to_csv(val_path, sep=\";\",header=False, index=False)\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## uncomment the code below to generate the text files for your train, val, and test datasets.\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_train, input_val, target_train, target_val = train_test_split(data.text.to_numpy(), \n",
    "                                                                    data.emotions.to_numpy(), \n",
    "                                                                    test_size=0.2)\n",
    "\n",
    "# Split the validataion further to obtain a holdout dataset (for testing) -- split 50:50\n",
    "input_val, input_test, target_val, target_test = train_test_split(input_val, target_val, test_size=0.5)\n",
    "\n",
    "\n",
    "## create a dataframe for each dataset\n",
    "train_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\n",
    "val_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\n",
    "test_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\n",
    "final_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n",
    "\n",
    "train_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\n",
    "val_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\n",
    "test_dataset.to_csv(val_path, sep=\";\",header=False, index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Dataset object that will be used to load the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.data_column = \"text\"\n",
    "        self.class_column = \"class\"\n",
    "        self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                               engine=\"python\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i now feel compromised and skeptical of the value of every unit of work i put in',\n",
       " 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = EmoDataset(train_path)\n",
    "ds[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with PyTorchLightning\n",
    "\n",
    "[PyTorchLightning](https://www.pytorchlightning.ai/) is a library that abstracts the complexity of training neural networks with PyTorch. It is built on top of PyTorch and simplifies training.\n",
    "\n",
    "![](https://pytorch-lightning.readthedocs.io/en/latest/_images/pt_to_pl.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methods required by PyTorchLightning\n",
    "\n",
    "class TrainingModule(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "        self.loss = nn.CrossEntropyLoss() ## combines LogSoftmax() and NLLLoss()\n",
    "        #self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(X), y)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        return self.model(X, *args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"train\")\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"val_loss\": loss}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.val_path)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_data_loader(self.hparams.test_path)\n",
    "                \n",
    "    def create_data_loader(self, ds_path: str, shuffle=False):\n",
    "        return DataLoader(\n",
    "                    EmoDataset(ds_path),\n",
    "                    batch_size=self.hparams.batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    collate_fn=TokenizersCollateFn()\n",
    "        )\n",
    "        \n",
    "    @lru_cache()\n",
    "    def total_steps(self):\n",
    "        return len(self.train_dataloader()) // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ## use AdamW optimizer -- faster approach to training NNs\n",
    "        ## read: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Learning rate for the model\n",
    "\n",
    "The code below aims to obtain valuable information about the optimal learning rate during a pretraining run. Determine boundary and increase the leanring rate linearly or exponentially.\n",
    "\n",
    "More: https://github.com/davidtvs/pytorch-lr-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_training.pkl  sample_data  tokenizer  val.txt\n",
      "pytorch-lr-finder    test.txt\t  train.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing torch_lr_finder.egg-info/PKG-INFO\n",
      "writing dependency_links to torch_lr_finder.egg-info/dependency_links.txt\n",
      "writing requirements to torch_lr_finder.egg-info/requires.txt\n",
      "writing top-level names to torch_lr_finder.egg-info/top_level.txt\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'torch_lr_finder.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/torch_lr_finder\n",
      "copying build/lib/torch_lr_finder/__init__.py -> build/bdist.linux-x86_64/egg/torch_lr_finder\n",
      "copying build/lib/torch_lr_finder/lr_finder.py -> build/bdist.linux-x86_64/egg/torch_lr_finder\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch_lr_finder/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/torch_lr_finder/lr_finder.py to lr_finder.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying torch_lr_finder.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying torch_lr_finder.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying torch_lr_finder.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying torch_lr_finder.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying torch_lr_finder.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/torch_lr_finder-0.2.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing torch_lr_finder-0.2.1-py3.7.egg\n",
      "Removing /usr/local/lib/python3.7/dist-packages/torch_lr_finder-0.2.1-py3.7.egg\n",
      "Copying torch_lr_finder-0.2.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
      "torch-lr-finder 0.2.1 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /usr/local/lib/python3.7/dist-packages/torch_lr_finder-0.2.1-py3.7.egg\n",
      "Processing dependencies for torch-lr-finder==0.2.1\n",
      "Searching for packaging==21.3\n",
      "Best match: packaging 21.3\n",
      "Adding packaging 21.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for tqdm==4.64.0\n",
      "Best match: tqdm 4.64.0\n",
      "Adding tqdm 4.64.0 to easy-install.pth file\n",
      "Installing tqdm script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for torch==1.11.0+cu113\n",
      "Best match: torch 1.11.0+cu113\n",
      "Adding torch 1.11.0+cu113 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /usr/local/bin\n",
      "Installing convert-onnx-to-caffe2 script to /usr/local/bin\n",
      "Installing torchrun script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for numpy==1.21.6\n",
      "Best match: numpy 1.21.6\n",
      "Adding numpy 1.21.6 to easy-install.pth file\n",
      "Installing f2py script to /usr/local/bin\n",
      "Installing f2py3 script to /usr/local/bin\n",
      "Installing f2py3.7 script to /usr/local/bin\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for matplotlib==3.2.2\n",
      "Best match: matplotlib 3.2.2\n",
      "Adding matplotlib 3.2.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for pyparsing==3.0.9\n",
      "Best match: pyparsing 3.0.9\n",
      "Adding pyparsing 3.0.9 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for typing-extensions==4.1.1\n",
      "Best match: typing-extensions 4.1.1\n",
      "Adding typing-extensions 4.1.1 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for kiwisolver==1.4.3\n",
      "Best match: kiwisolver 1.4.3\n",
      "Adding kiwisolver 1.4.3 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for cycler==0.11.0\n",
      "Best match: cycler 0.11.0\n",
      "Adding cycler 0.11.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Searching for six==1.15.0\n",
      "Best match: six 1.15.0\n",
      "Adding six 1.15.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/local/lib/python3.7/dist-packages\n",
      "Finished processing dependencies for torch-lr-finder==0.2.1\n"
     ]
    }
   ],
   "source": [
    "!cd pytorch-lr-finder && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t examples  README.md  torch_lr_finder\n",
      "CONTRIBUTING.md  images    setup.py   torch_lr_finder.egg-info\n",
      "dist\t\t LICENSE   tests\n"
     ]
    }
   ],
   "source": [
    "!cd pytorch-lr-finder/ && ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch_lr_finder (below 3 blocks) has import error -> not working when i actually run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_lr_finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "lr=0.1 ## uper bound LR\n",
    "#from torch_lr_finder import LRFinder\n",
    "hparams_tmp = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    epochs=1,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1,\n",
    ")\n",
    "module = TrainingModule(hparams_tmp)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(module.parameters(), lr=5e-7) ## lower bound LR\n",
    "# lr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\n",
    "# lr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100, accumulation_steps=hparams_tmp.accumulate_grad_batches)\n",
    "# lr_finder.plot()\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-4 \n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_finder.plot(show_lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Emotion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is what we actually ended up running, don't be confused by the above codes which looked like the one here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    epochs=1,\n",
    "    lr=lr,\n",
    "    accumulate_grad_batches=1\n",
    ")\n",
    "module = TrainingModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## garbage collection\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /content/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | EmoModel         | 82.1 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "82.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "82.1 M    Total params\n",
      "328.492   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087232ec10554d10b087fbeab4a8926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb755ec248c412495f1145a564b9ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d868a8b588d4a8c8ad56ca004399482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## train roughly for about 10-15 minutes with GPU enabled.\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n",
    "                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n",
    "\n",
    "trainer.fit(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness   0.965458  0.962134  0.963793       581\n",
      "         joy   0.957101  0.930935  0.943837       695\n",
      "        love   0.787709  0.886792  0.834320       159\n",
      "       anger   0.902778  0.945455  0.923623       275\n",
      "        fear   0.893805  0.901786  0.897778       224\n",
      "    surprise   0.826923  0.651515  0.728814        66\n",
      "\n",
      "    accuracy                       0.926000      2000\n",
      "   macro avg   0.888962  0.879770  0.882027      2000\n",
      "weighted avg   0.927207  0.926000  0.925894      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "    module.eval()\n",
    "    true_y, pred_y = [], []\n",
    "    for i, batch_ in enumerate(module.test_dataloader()):\n",
    "        (X, attn), y = batch_\n",
    "        #batch = (X.cuda(), attn.cuda()) # who put this stupid line here, which messed up the devices\n",
    "        batch = X, attn\n",
    "        print(progress[i % len(progress)], end=\"\\r\")\n",
    "        # but now it takes forever to calculate a single epoch\n",
    "        y_pred = torch.argmax(module(batch), dim=1)\n",
    "        true_y.extend(y.cpu())\n",
    "        pred_y.extend(y_pred.cpu())\n",
    "print(\"\\n\" + \"_\" * 80)\n",
    "print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=len(emotions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's actually save the result in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive/MyDrive/AI_sheng/emo_0.pt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this would throw an error, we have to \"mount the drive\" first, see the cells below\n",
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always remember to change the newly saved file name to avoid over-writing the previous records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"drive/MyDrive/AI_sheng/emo_0_result.txt\", \"w\") as f:\n",
    "  for line in classification_report(true_y, pred_y, target_names=label2int.keys(), digits=len(emotions)):\n",
    "    f.write(line)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul  4 13:50:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   72C    P0    33W /  70W |   1428MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat we need is to save the trained model (weights &)\\n\\n\\nto save model:\\n  module.model (?) # see TrainingModule class\\n\\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What we need is to save the trained model (weights &)\n",
    "\n",
    "\n",
    "to save model:\n",
    "  module.model (?) # see TrainingModule class\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# to load the model, the class definition would not be saved in the file\n",
    "# and therefore should be recorded somewhere else\n",
    "PATH = ...\n",
    "torch.save(module.model.state_dict(), PATH)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify if it works\n",
    "\"\"\"\n",
    "model = ...\n",
    "model.load(PATH)\n",
    "model.eval() # important to not change the params in the model\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÄåRoBERTa_Fine_Tuning_Emotion_classification.ipynb„ÄçÁöÑÂâØÊú¨\n"
     ]
    }
   ],
   "source": [
    "!ls drive/MyDrive/AI_sheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "base_model.embeddings.position_ids \t torch.Size([1, 514])\n",
      "base_model.embeddings.word_embeddings.weight \t torch.Size([50265, 768])\n",
      "base_model.embeddings.position_embeddings.weight \t torch.Size([514, 768])\n",
      "base_model.embeddings.token_type_embeddings.weight \t torch.Size([1, 768])\n",
      "base_model.embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "base_model.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "base_model.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "base_model.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "base_model.encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "base_model.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "base_model.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "classifier.1.weight \t torch.Size([768, 768])\n",
      "classifier.1.bias \t torch.Size([768])\n",
      "classifier.4.weight \t torch.Size([6, 768])\n",
      "classifier.4.bias \t torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in module.model.state_dict():\n",
    "    print(param_tensor, \"\\t\", module.model.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the whole damn thing, not just the weight, but the hyperparams, loss, etc\n",
    "# Additional information\n",
    "EPOCH = 1\n",
    "PATH = \"drive/MyDrive/AI_sheng/emo_0.pt\"\n",
    "LOSS = 0.\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': module.model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmoModel(\n",
       "  (base_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.05, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): Mish()\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test load model\n",
    "# we need the architecture\n",
    "model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for inference only as well to reduce the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive/MyDrive/AI_sheng/emo_0.pt'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(module.model, 'drive/MyDrive/AI_sheng/emo_0_inference_only.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EmoModel(\n",
       "  (base_model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.05, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): Mish()\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test load model\n",
    "# we need the architecture\n",
    "model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "checkpoint = torch.load(\"drive/MyDrive/AI_sheng/emo_0.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   0,  118,  376,  ...,    1,    1,    1],\n",
       "         [   0,  118,   21,  ...,    1,    1,    1],\n",
       "         [   0,  757, 2157,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   0,  118,  619,  ...,    1,    1,    1],\n",
       "         [   0,  757, 2157,  ...,    1,    1,    1],\n",
       "         [   0,  118,  619,  ...,    1,    1,    1]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previously used test batch to test if the reloaded model works\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9080, -1.2302, -0.9013,  5.8108, -0.3313, -1.6975],\n",
       "        [-2.0946,  6.4397,  0.3476, -1.8304, -1.9126, -1.6221],\n",
       "        [-1.0167, -1.4537, -2.5131, -0.5782,  6.0331, -0.0138],\n",
       "        [ 6.1555, -0.9121, -1.5822,  0.0782, -0.4525, -1.8345],\n",
       "        [ 6.1081, -0.0588, -1.4681, -0.5297, -0.9191, -1.9302],\n",
       "        [ 5.9176, -0.2496, -0.8827,  0.4110, -1.6411, -2.4032],\n",
       "        [-1.5229,  2.6171,  4.4723, -2.0810, -2.4527, -2.6025],\n",
       "        [-1.4750,  2.2911,  4.3958, -1.4681, -2.4978, -2.7022],\n",
       "        [-2.1249, -1.7336, -1.9561,  2.9887,  2.9735,  0.1342],\n",
       "        [-0.8134, -1.6481, -1.5137,  5.8931,  0.3576, -1.3272],\n",
       "        [ 5.9923, -1.1148, -1.9877, -0.3512,  0.3510, -1.4085],\n",
       "        [ 1.0029,  0.2793,  1.0631,  3.3292, -2.5348, -2.9683],\n",
       "        [-0.6613, -1.4232, -1.0356,  5.8153, -0.2930, -1.5194],\n",
       "        [-1.8507,  6.3146, -0.0732, -1.9502, -2.2076, -0.7117],\n",
       "        [-1.8714,  6.5112,  0.0234, -2.0143, -1.5364, -1.5565],\n",
       "        [-1.5251, -0.9851, -2.0907, -1.8387,  3.6508,  3.0799]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model reload works (inference only works)\n",
    "model.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below codes are newly added for exporting & api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's just start from here and export the passed codes\n",
    "#### don't train the whole model unless it's necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting this won't work\n",
    "# since they're locally imported\n",
    "def setup_imports():\n",
    "    \"\"\"\n",
    "    import everything is slow, try to fix it after we nailed down the data classes\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from typing import List\n",
    "    import torch.nn.functional as F\n",
    "    from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "    import logging\n",
    "    import os\n",
    "    from functools import lru_cache\n",
    "    from tokenizers import ByteLevelBPETokenizer\n",
    "    from tokenizers.processors import BertProcessing\n",
    "    import pytorch_lightning as pl\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    import pandas as pd\n",
    "    from argparse import Namespace\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(torch.__version__)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# necessary evil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EmoModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
    "        # customize here\n",
    "        # use the <s> representation\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the label2int map since it's used everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "label2int = {\n",
    "    \"sadness\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"love\": 2,\n",
    "    \"anger\": 3,\n",
    "    \"fear\": 4,\n",
    "    \"surprise\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_model(PATH, pretrained=True, inference_only=True, lr=0.0001, default_model=None):\n",
    "    if PATH[-3:] != \".pt\" and PATH[-4:] != \".pth\":\n",
    "        print(\"Unable to load pretrained model\")\n",
    "        return None\n",
    "\n",
    "    # show warning message when it's inference only but lr has been changed\n",
    "    if inference_only == True and lr != 0.0001:\n",
    "        print(\"Warning: the loaded model is for inference only, so there's no optimizer for the changed learning rate\")\n",
    "    # model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "    # emotions = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"], len(label2int) would work as well\n",
    "    model = None\n",
    "    if default_model is None:\n",
    "        from transformers import AutoModelWithLMHead\n",
    "        model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, 6)\n",
    "    else:\n",
    "        # set model to user-defined model\n",
    "        model = default_model\n",
    "\n",
    "    # if you want to train it from scratch\n",
    "    if pretrained == False:\n",
    "        return model\n",
    "\n",
    "    checkpoint = torch.load(PATH)\n",
    "\n",
    "    if inference_only:\n",
    "        # model would not be subscriptable\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "    else:\n",
    "        # lr: learning rate, adjustable\n",
    "        from transformers import AdamW\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For inference only, we should just take advantage of the class TokenizersCollateFN() class defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# save the pretrained token\n",
    "def load_tokenizer():\n",
    "    from tokenizers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    # -p, --parent, no error if it exists, make parent directory as necessary\n",
    "    os.system(\"mkdir -p tokenizer\")\n",
    "    tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "def setup_tokenizer():\n",
    "    # if there's no previous file/record\n",
    "    # should we check if there are missing files given that it's previously downloaded?\n",
    "    if not os.path.isdir(\"tokenizer\"):\n",
    "        load_tokenizer()\n",
    "\n",
    "    else: # content of previously download files is not complete\n",
    "        checklist = ['merges.txt', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n",
    "        # check existing files\n",
    "        # existing_files = os.walk(os.path.join(os.getcwd(), \"tokenizer\"))\n",
    "        existing_files = list(os.walk(\"tokenizer\"))[0][2]\n",
    "        # os.walk() won't messed up the order of the searched files,\n",
    "        # so, we can just use \"==\" operator\n",
    "        if existing_files != checklist:\n",
    "            # clean the previously download ones\n",
    "            os.system(\"rmdir -rf tokenizer\")\n",
    "            # and, re-download it\n",
    "            load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_emotion(output, print_emo=True):\n",
    "    # output = model.forward(input)\n",
    "    import torch\n",
    "    idx = torch.argmax(output, dim=1)\n",
    "    from EMO_AI.model_api import label2int\n",
    "    for key in label2int:\n",
    "        if label2int[key] == idx:\n",
    "            if print_emo == True:\n",
    "                print(\"Emotion: %s\" % key)\n",
    "            break\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above are stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EMO_AI.model_api import *\n",
    "from EMO_AI.data_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# change this\n",
    "PATH = \"C:\"+chr(92)+\"Users\"+chr(92)+\"Kelvin\"+chr(92)+\"Desktop\"+chr(92)+\"ai_sheng\"+chr(92)+\"emo_0.pt\"\n",
    "model = get_model(PATH, inference_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This love has taken it's toll on me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we add tokenizer as an attribute of the model\n",
    "### for potential more concise high-level API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to lessen code to get model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# mainly about not re-writing with torch.no_grad(), model.eval() again\n",
    "def get_output(text, model, tokenizer=None, return_tensor=False, print_emo=False):\n",
    "    # we should add try/Except error handling for \"model\" argument\n",
    "    # , but i consider it to be ugly\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # we have control flow in convert_text_to_tensor()\n",
    "        out = model(convert_text_to_tensor(text, tokenizer))\n",
    "    # put it right here to enable \"print_emo\" argument\n",
    "    emo_label = print_emotion(out, print_emo=print_emo)\n",
    "    if return_tensor == True:\n",
    "        return out\n",
    "    # else, return emotion label (a string)\n",
    "    return emo_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: sadness\n",
      "sadness tensor([[ 2.2319,  0.6387,  0.6452,  1.6908, -1.7631, -3.2787]])\n"
     ]
    }
   ],
   "source": [
    "result = get_output(text, model, tokenizer)\n",
    "# deliberately not to use defined tokenizer\n",
    "result_tensor = get_output(text, model, return_tensor=True, print_emo=True)\n",
    "print(result, result_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('AI_sheng')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
