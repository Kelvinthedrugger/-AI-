{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# necessary evil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmoModel(nn.Module):\n",
    "    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, base_model_output_size),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(base_model_output_size, n_classes)\n",
    "        )\n",
    "\n",
    "        for layer in self.classifier:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if layer.bias is not None:\n",
    "                    layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X, attention_mask = input_\n",
    "        hidden_states = self.base_model(X, attention_mask=attention_mask)\n",
    "        # customize here\n",
    "        # use the <s> representation\n",
    "        return self.classifier(hidden_states[0][:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the label2int map since it's used everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "label2int = {\n",
    "    \"sadness\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"love\": 2,\n",
    "    \"anger\": 3,\n",
    "    \"fear\": 4,\n",
    "    \"surprise\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model(PATH=\".pt\", pretrained=True, inference_only=True, lr=0.0001, default_model=None):\n",
    "    if PATH[-3:] != \".pt\" and PATH[-4:] != \".pth\":\n",
    "        print(\"Unable to load pretrained model\")\n",
    "        return None\n",
    "\n",
    "    # show warning message when it's inference only but lr has been changed\n",
    "    if inference_only == True and lr != 0.0001:\n",
    "        print(\"Warning: the loaded model is for inference only, so there's no optimizer for the changed learning rate\")\n",
    "    # model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n",
    "    # emotions = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"], len(label2int) would work as well\n",
    "    model = None\n",
    "    if default_model is None:\n",
    "        from transformers import AutoModelWithLMHead\n",
    "        model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, 6)\n",
    "    else:\n",
    "        # set model to user-defined model\n",
    "        model = default_model\n",
    "\n",
    "    # if you want to train it from scratch\n",
    "    if pretrained == False:\n",
    "        return model\n",
    "\n",
    "    checkpoint = torch.load(PATH)\n",
    "\n",
    "    if inference_only:\n",
    "        # model would not be subscriptable\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model.eval()\n",
    "    else:\n",
    "        # lr: learning rate, adjustable\n",
    "        from transformers import AdamW\n",
    "        optimizer = AdamW(model.parameters(), lr=lr)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporarily fix get_model() to allow customed random model (not pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EMO_AI.data_process import convert_text_to_tensor\n",
    "from EMO_AI.model_api import mish, Mish, EmoModel, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0280,  0.1436,  0.1584,  0.0828,  0.0293,  0.0380]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(pretrained=False)\n",
    "ret = model(convert_text_to_tensor(\"We are the world\"))\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For inference only, we should just take advantage of the class TokenizersCollateFN() class defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# save the pretrained token\n",
    "def load_tokenizer():\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "    # -p, --parent, no error if it exists, make parent directory as necessary\n",
    "    import os\n",
    "    os.system(\"mkdir -p tokenizer\")\n",
    "    tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "def setup_tokenizer():\n",
    "    import os\n",
    "    # if there's no previous file/record\n",
    "    # should we check if there are missing files given that it's previously downloaded?\n",
    "    if not os.path.isdir(\"tokenizer\"):\n",
    "        load_tokenizer()\n",
    "\n",
    "    else: # content of previously download files is not complete\n",
    "        checklist = ['merges.txt', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'vocab.json']\n",
    "        # check existing files\n",
    "        # existing_files = os.walk(os.path.join(os.getcwd(), \"tokenizer\"))\n",
    "        existing_files = list(os.walk(\"tokenizer\"))[0][2]\n",
    "        # os.walk() won't messed up the order of the searched files,\n",
    "        # so, we can just use \"==\" operator\n",
    "        if existing_files != checklist:\n",
    "            # clean the previously download ones\n",
    "            os.system(\"rmdir -rf tokenizer\")\n",
    "            # and, re-download it\n",
    "            load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def print_emotion(output, print_emo=True):\n",
    "    # output = model.forward(input)\n",
    "    import torch\n",
    "    idx = torch.argmax(output, dim=1)\n",
    "    from EMO_AI.model_api import label2int\n",
    "    for key in label2int:\n",
    "        if label2int[key] == idx:\n",
    "            if print_emo == True:\n",
    "                print(\"Emotion: %s\" % key)\n",
    "            break\n",
    "    return key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above are stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EMO_AI.model_api import *\n",
    "# from EMO_AI.data_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "c:\\Users\\Kelvin\\anaconda3\\envs\\AI_sheng\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# change this\n",
    "PATH = \"C:\"+chr(92)+\"Users\"+chr(92)+\"Kelvin\"+chr(92)+\"Desktop\"+chr(92)+\"ai_sheng\"+chr(92)+\"emo_0.pt\"\n",
    "model = get_model(PATH, inference_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This love has taken it's toll on me\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we add tokenizer as an attribute of the model\n",
    "### for potential more concise high-level API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to lessen code to get model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# mainly about not re-writing with torch.no_grad(), model.eval() again\n",
    "def get_output(text, model, tokenizer=None, return_tensor=False, print_emo=False):\n",
    "    # we should add try/Except error handling for \"model\" argument\n",
    "    # , but i consider it to be ugly\n",
    "    from EMO_AI.data_process import convert_text_to_tensor\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # we have control flow in convert_text_to_tensor()\n",
    "        out = model(convert_text_to_tensor(text, tokenizer))\n",
    "    # put it right here to enable \"print_emo\" argument\n",
    "    emo_label = print_emotion(out, print_emo=print_emo)\n",
    "    if return_tensor == True:\n",
    "        return out\n",
    "    # else, return emotion label (a string)\n",
    "    return emo_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: sadness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2319,  0.6387,  0.6452,  1.6908, -1.7631, -3.2787]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deliberately not to use defined tokenizer\n",
    "result_tensor = get_output(text, model, return_tensor=True, print_emo=True)\n",
    "result_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('AI_sheng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db77c7d38baf4b93e998a2213b9026a3d1545f748ab5419893b25caa4e3cb58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
