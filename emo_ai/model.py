# AUTOGENERATED! DO NOT EDIT! File to edit: 01_modelAPI.ipynb (unless otherwise specified).

__all__ = ['tokenizer', 'mish', 'Mish', 'EmoModel', 'TokenizersCollateFn', 'get_pretrained_model']

# Cell
# the robertA model (example from the author)
import torch
from torch import nn
from typing import List
import torch.nn.functional as F
from transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
import logging
import os
from functools import lru_cache
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing
import pytorch_lightning as pl
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from argparse import Namespace
from sklearn.metrics import classification_report
# torch.__version__

# Cell
tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')

# Cell
# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py
@torch.jit.script
def mish(input):
    return input * torch.tanh(F.softplus(input))

class Mish(nn.Module):
    def forward(self, input):
        return mish(input)

# Cell
class EmoModel(nn.Module):
    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):
        super().__init__()
        self.base_model = base_model

        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(base_model_output_size, base_model_output_size),
            Mish(),
            nn.Dropout(dropout),
            nn.Linear(base_model_output_size, n_classes)
        )

        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                layer.weight.data.normal_(mean=0.0, std=0.02)
                if layer.bias is not None:
                    layer.bias.data.zero_()

    def forward(self, input_, *args):
        X, attention_mask = input_
        hidden_states = self.base_model(X, attention_mask=attention_mask)

        # maybe do some pooling / RNNs... go crazy here!

        # use the <s> representation
        return self.classifier(hidden_states[0][:, 0, :])

# Cell
# needed for tokenize the user input
class TokenizersCollateFn:
    def __init__(self, max_tokens=512):

        # I still need this to parse the input
        # try to figure out where to store these tokens
        # instead of re-download it every time
        # reload is probably fine if the model is on AWS?
        ## RoBERTa uses BPE tokenizer similar to GPT
        t = ByteLevelBPETokenizer(
            "tokenizer/vocab.json",
            "tokenizer/merges.txt"
        )
        t._tokenizer.post_processor = BertProcessing(
            ("</s>", t.token_to_id("</s>")),
            ("<s>", t.token_to_id("<s>")),
        )
        t.enable_truncation(max_tokens)
        t.enable_padding(length=max_tokens, pad_id=t.token_to_id("<pad>"))
        self.tokenizer = t

    def __call__(self, batch):
        encoded = self.tokenizer.encode_batch([x[0] for x in batch])
        sequences_padded = torch.tensor([enc.ids for enc in encoded])
        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])
        labels = torch.tensor([x[1] for x in batch])

        return (sequences_padded, attention_masks_padded), labels

# Cell
def get_pretrained_model(PATH, inference_only=True):
    if PATH[-3:] != ".pt" and PATH[-4:] != ".pth":
        print("Unable to load pretrained model")
        return None

    # model = EmoModel(AutoModelWithLMHead.from_pretrained("distilroberta-base").base_model, len(emotions))
    # see above cell: emotions = ["sadness", "joy", "love", "anger", "fear", "surprise"]
    model = EmoModel(AutoModelWithLMHead.from_pretrained("distilroberta-base").base_model, 6)
    # lr: learning rate, adjustable
    optimizer = AdamW(model.parameters(), lr=0.0001)

    if inference_only:
        model.load_state_dict(PATH)
    else:
        checkpoint = torch.load(PATH)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    model.eval() # or model.train()
    return model

# Cell
# save the pretrained token
# try not to re-download it every time
# !mkdir -p tokenizer
if not os.path.isdir("tokenizer"):
    os.system("mkdir tokenizer")

tokenizer.save_pretrained("tokenizer")